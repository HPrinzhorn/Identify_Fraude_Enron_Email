{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############LOADING PACKAGES...    #####################\n",
      "#############LOADING VISUALIZATION LIBRARIES... #########\n",
      "#############LOADING SCIKIT-LEARN PACKAGES... ############\n",
      "############# LOAD NESTED DICTIONARY - DATASET... #######\n",
      "############# AND CREATING DATAFRAME... #################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THB4UT\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## DELETING THE FOLLOWING FEATURES... #############\n",
      "['loan_advances', 'director_fees', 'restricted_stock_deferred', 'deferral_payments']\n",
      "#########################################################\n",
      "############# DROPING THE FOLLOWING OUTLIERS...  ########\n",
      "['TOTAL', 'THE TRAVEL AGENCY IN THE PARK', 'LAVORATO JOHN J', 'FREVERT MARK A', 'ALLEN PHILLIP K', 'LOCKHART EUGENE E', 'MCMAHON JEFFREY', 'FALLON JAMES B', 'KITCHEN LOUISE', 'WHALLEY LAWRENCE G', 'SHANKMAN JEFFREY A', 'HICKERSON GARY J']\n",
      "############# CREATING NEW FEATURES... ##################\n",
      "#########################################################\n",
      "############# FEATURES LIST... ######################\n",
      "#########################################################\n",
      "['poi', 'bonus', 'expenses', 'exercised_stock_options']\n",
      "#########################################################\n",
      "############# CREATE MY_DATASET... ######################\n",
      "#########################################################\n",
      "#############b############################################\n",
      "############# CREATE LABELS AND FEATURES... #############\n",
      "#########################################################\n",
      "#########################################################\n",
      "############# FEATURE SCALING ...   #####################\n",
      "#########################################################\n",
      "#########################################################\n",
      "###### FEATURE SELECTION ... ############################\n",
      "###### PARAMETER GRID ... ###############################\n",
      "#########################################################\n",
      "###### EXAMINE FIRST TUPLE OF SCORES... #################\n",
      "#########################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THB4UT\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\sklearn\\utils\\__init__.py:127: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dt__criterion': 'gini', 'dt__max_depth': 3, 'kbest__k': 1, 'dt__min_samples_split': 10, 'dt__splitter': 'best'}\n",
      "[0.92       0.92       0.88       0.95833333 0.875     ]\n",
      "0.9105691056910569\n",
      "#########################################################\n",
      "###### CREATE A LIST WITH THE MEAN SCORES... ############\n",
      "#########################################################\n",
      "#########################################################\n",
      "###### EXAMINE THE BEST MODEL... ########################\n",
      "#########################################################\n",
      "0.926829268292683\n",
      "{'dt__criterion': 'entropy', 'dt__max_depth': 6, 'kbest__k': 3, 'dt__min_samples_split': 10, 'dt__splitter': 'random'}\n",
      "Pipeline(steps=[('kbest', SelectKBest(k=3, score_func=<function f_classif at 0x0EF971F0>)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='random'))])\n",
      "#########################################################\n",
      "###### FEATURES LIST FOR THE TESTER... ##################\n",
      "#########################################################\n",
      "['poi', 'bonus', 'expenses', 'exercised_stock_options']\n",
      "#########################################################\n",
      "###### PRINT CLASSIFICATION REPORT... ###################\n",
      "#########################################################\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.92      0.99      0.95       105\n",
      "        1.0       0.90      0.50      0.64        18\n",
      "\n",
      "avg / total       0.92      0.92      0.91       123\n",
      "\n",
      "#########################################################\n",
      "####### GETTING FEATURE SCORES... #######################\n",
      "#########################################################\n",
      "#########################################################\n",
      "####### RUNNING TESTER.PY ###############################\n",
      "#########################################################\n",
      "Pipeline(steps=[('kbest', SelectKBest(k=3, score_func=<function f_classif at 0x0EF971F0>)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='random'))])\n",
      "Got a divide by zero when trying out: Pipeline(steps=[('kbest', SelectKBest(k=3, score_func=<function f_classif at 0x0EF971F0>)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='random'))])\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "print(\"#############LOADING PACKAGES...    #####################\")\n",
    "from __future__ import division\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\\\\tools\") # see tester.py\n",
    "import pickle\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "print(\"#############LOADING VISUALIZATION LIBRARIES... #########\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#######################################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "print(\"#############LOADING SCIKIT-LEARN PACKAGES... ############\")\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.grid_search import GridSearchCV    # --> now under model_selection\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit#, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Imputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from ml_functions import *\n",
    "\n",
    "\n",
    "print(\"############# LOAD NESTED DICTIONARY - DATASET... #######\")\n",
    "print(\"############# AND CREATING DATAFRAME... #################\")\n",
    "data_dict = load_dict(\"final_project_dataset.pkl\")\n",
    "df = dict_to_df(data_dict)\n",
    "\n",
    "df[\"bin_email_address\"] = np.where(df[\"email_address\"]!=\"NaN\",1,0)\n",
    "\n",
    "df = cols_to_numeric(df, \"email_address\")\n",
    "# show_null_values(df, False)\n",
    "\n",
    "\n",
    "print(\"######## DELETING THE FOLLOWING FEATURES... #############\")\n",
    "lst = [\"loan_advances\", 'director_fees', 'restricted_stock_deferred',\n",
    "\"deferral_payments\"]\n",
    "print(lst)\n",
    "df = drop_bad_features(df, lst)\n",
    "print(\"#########################################################\")\n",
    "print(\"############# DROPING THE FOLLOWING OUTLIERS...  ########\")\n",
    "\n",
    "lst = ['TOTAL' , 'THE TRAVEL AGENCY IN THE PARK', 'LAVORATO JOHN J',\n",
    "      \"FREVERT MARK A\", \"ALLEN PHILLIP K\", \"LOCKHART EUGENE E\", \n",
    "       \"MCMAHON JEFFREY\", \"FALLON JAMES B\", \"KITCHEN LOUISE\", \n",
    "       \"WHALLEY LAWRENCE G\", \"SHANKMAN JEFFREY A\", \"HICKERSON GARY J\"]\n",
    "print(lst)\n",
    "df = drop_outliers(df, lst)\n",
    "print(\"############# CREATING NEW FEATURES... ##################\")\n",
    "df['ratio_from_poi'] = df.from_poi_to_this_person / df.to_messages\n",
    "df['ratio_to_poi'] = df.from_this_person_to_poi / df.from_messages\n",
    "df['log_ratio_to_poi'] = df[\"ratio_to_poi\"].apply(take_log_on_non_zero_entries_of_this_feature)\n",
    "df[\"log_ratio_from_poi\"] = df[\"ratio_from_poi\"].apply(take_log_on_non_zero_entries_of_this_feature)\n",
    "df = df.replace(\"NaN\", 0)\n",
    "print(\"#########################################################\")\n",
    "print(\"############# FEATURES LIST... ######################\")\n",
    "print(\"#########################################################\")\n",
    "features_dict = {\"SelectKBest\": [\"poi\", \"salary\", \"ratio_to_poi\",\n",
    "                                \"total_stock_value\", \n",
    "                                 \"exercised_stock_options\", \n",
    "                                \"bonus\", \"deferred_income\", \n",
    "                                \"total_payments\", \"restricted_stock\", \"other\"],\n",
    "                \"Lasso\": [\"poi\", \"from_poi_to_this_person\", \n",
    "                         \"from_this_person_to_poi\",\n",
    "                         \"shared_receipt_with_poi\",\n",
    "                         \"total_stock_value\"],\n",
    "                 \"RandomForest\": ['poi', 'shared_receipt_with_poi', 'salary', \n",
    "                                  'exercised_stock_options', 'from_poi_to_this_person', \n",
    "                                  'other', 'from_this_person_to_poi', 'deferred_income', \n",
    "                                  'exercised_stock_options', 'expenses', \n",
    "                                  'long_term_incentive', 'restricted_stock'],\n",
    "                 \"DecisionTree\": [\"poi\", \"bonus\", \"expenses\", \n",
    "                       \"exercised_stock_options\", \"restricted_stock\"],\n",
    "                 \"all\": [\"poi\", 'bonus', 'deferred_income', 'bin_email_address',\n",
    "                         'exercised_stock_options',  'expenses', 'from_messages',\n",
    "                         'from_poi_to_this_person', 'from_this_person_to_poi',\n",
    "                         'long_term_incentive', 'other', 'restricted_stock',\n",
    "                         'salary', 'shared_receipt_with_poi', 'to_messages',\n",
    "                         'total_payments', 'total_stock_value', 'ratio_from_poi',\n",
    "                         'ratio_to_poi', 'log_ratio_to_poi', 'log_ratio_from_poi'],\n",
    "                 \"RF_clf_on_DT_list\": ['poi', 'bonus', 'expenses', \n",
    "                                       'exercised_stock_options', 'restricted_stock'],\n",
    "                 \"RF_clf_on_DT_list2\": ['poi', 'bonus', 'expenses', 'exercised_stock_options']\n",
    "                }\n",
    "###########################################################################################\n",
    "features_list = features_dict[\"RF_clf_on_DT_list2\"]\n",
    "###########################################################################################\n",
    "print(features_list)\n",
    "print(\"#########################################################\")\n",
    "print(\"############# CREATE MY_DATASET... ######################\")\n",
    "print(\"#########################################################\")\n",
    "df = df.replace(np.nan, 0)\n",
    "my_dataset = transpose_to_dict(df)\n",
    "\n",
    "print(\"#############b############################################\")\n",
    "print(\"############# CREATE LABELS AND FEATURES... #############\")\n",
    "print(\"#########################################################\")\n",
    "labels, features = labels_features(my_dataset, features_list, True)\n",
    "\n",
    "print(\"#########################################################\")\n",
    "print(\"############# FEATURE SCALING ...   #####################\")\n",
    "print(\"#########################################################\")\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "features = scaler.transform(features)\n",
    "df_feat = pd.DataFrame(features, columns = features_list[1:])\n",
    "\n",
    "print(\"#########################################################\")\n",
    "print(\"###### FEATURE SELECTION ... ############################\")\n",
    "print(\"###### PARAMETER GRID ... ###############################\")\n",
    "kbest = SelectKBest(f_classif)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state = 42)\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "ada = AdaBoostClassifier()\n",
    "lr = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                    (\"kbest\", kbest),\n",
    "                     (\"dt\", dt),\n",
    "                    ])\n",
    "            \n",
    "# rf: Parameters of RandomForest\n",
    "# Parameters of DecisionTree\n",
    "pg = {\"kbest__k\": [1,2,3], \n",
    "      \"dt__min_samples_split\": np.arange(10,120,30).tolist(), \n",
    "      \"dt__criterion\": [\"gini\", \"entropy\"],\n",
    "      \"dt__splitter\":[\"best\", \"random\"],\n",
    "      \"dt__max_depth\": [3, 6, 8, 11]},\n",
    "#            \"rf\": {\"n_estimators\": np.arange(20,220,40).tolist(), \n",
    "#                       \"min_samples_split\" : np.arange(2,12, 3).tolist(),\n",
    "#                       \"criterion\" : ['gini', 'entropy']},\n",
    "#            \"knn\": {'n_neighbors': np.arange(1,6).tolist(),\n",
    "#                       'weights': ['distance', 'uniform'],\n",
    "#                     'algorithm': ['kd_tree', 'ball_tree', 'auto', 'brute']},\n",
    "#            \"ada\": {'algorithm' : ['SAMME', 'SAMME.R'],\n",
    "#                    'n_estimators': [25, 50, 100],\n",
    "#                 'learning_rate': [.5, 1., 1.5],},\n",
    "#            \"lr\": {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "#            }\n",
    "\n",
    "###########################################################################################\n",
    "# clf = clf_dict[\"rf\"]\n",
    "# pg = pg_dict[\"rf\"]\n",
    "# print(pg_dict[\"rf\"])\n",
    "###########################################################################################\n",
    "print(\"#########################################################\")\n",
    "print(\"###### EXAMINE FIRST TUPLE OF SCORES... #################\")\n",
    "print(\"#########################################################\")\n",
    "# instantiate the grid\n",
    "grid = GridSearchCV(pipeline, pg, cv = 5, scoring = \"accuracy\")\n",
    "# fit the grid with data\n",
    "grid.fit(features, labels)\n",
    "\n",
    "# examine the first tuple\n",
    "print(grid.grid_scores_[0].parameters)\n",
    "print(grid.grid_scores_[0].cv_validation_scores)\n",
    "print(grid.grid_scores_[0].mean_validation_score)\n",
    "\n",
    "print(\"#########################################################\")\n",
    "print(\"###### CREATE A LIST WITH THE MEAN SCORES... ############\")\n",
    "print(\"#########################################################\")\n",
    "# grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
    "# print(grid_mean_scores)\n",
    "\n",
    "print(\"#########################################################\")\n",
    "print(\"###### EXAMINE THE BEST MODEL... ########################\")\n",
    "print(\"#########################################################\")\n",
    "# examine the best model\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# best estimator\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "#################################################################\n",
    "### Decision Tree importances\n",
    "################################################################\n",
    "# feat_list = features_list[1:]\n",
    "\n",
    "# print(\"#########################################################\")\n",
    "# print(\"###### GINI IMPORTANCE OF EACH FEATURE... ###############\")\n",
    "# print(\"#########################################################\")\n",
    "# # Print the name and gini importance of each feature\n",
    "# features_list = [\"poi\"]\n",
    "# features_importances = zip(feat_list, clf.feature_importances_)\n",
    "# for f, s in sorted(features_importances, key = lambda x:x[1], reverse = True):\n",
    "#     print('{:>25}: {:.3f}'.format(f, s))\n",
    "#     if s > 0.0:\n",
    "#         features_list.append(f)\n",
    "\n",
    "print(\"#########################################################\")\n",
    "print(\"###### FEATURES LIST FOR THE TESTER... ##################\")\n",
    "print(\"#########################################################\")\n",
    "print(features_list)\n",
    "\n",
    "df = df[features_list]\n",
    "my_dataset = transpose_to_dict(df)\n",
    "\n",
    "print(\"#########################################################\")\n",
    "print(\"###### PRINT CLASSIFICATION REPORT... ###################\")\n",
    "print(\"#########################################################\")\n",
    "report = classification_report(labels, clf.predict(features))\n",
    "print report\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "print(\"#########################################################\")\n",
    "print(\"####### GETTING FEATURE SCORES... #######################\")\n",
    "print(\"#########################################################\")\n",
    "# k = grid.get_params(True)['estimator__SelectKBest__transformer_list'][0][1]\n",
    "# features_scores = zip(features_list[1:], k.scores_)\n",
    "# for f, s in sorted(features_scores, key=lambda x: x[1], reverse=True):\n",
    "#     print('%s: %s'%(f, s))\n",
    "    \n",
    "    \n",
    "print(\"#########################################################\")\n",
    "print(\"####### RUNNING TESTER.PY ###############################\")\n",
    "print(\"#########################################################\")\n",
    "%run \"tester.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>AdaBoost</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>DecisionTree</th>\n",
       "      <th>K-NearestNeighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.63949</td>\n",
       "      <td>0.88508</td>\n",
       "      <td>0.90811</td>\n",
       "      <td>0.89200</td>\n",
       "      <td>0.84557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.11167</td>\n",
       "      <td>0.57968</td>\n",
       "      <td>0.74585</td>\n",
       "      <td>0.62972</td>\n",
       "      <td>0.43413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.23980</td>\n",
       "      <td>0.54420</td>\n",
       "      <td>0.48540</td>\n",
       "      <td>0.48740</td>\n",
       "      <td>0.47060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.15238</td>\n",
       "      <td>0.56138</td>\n",
       "      <td>0.58808</td>\n",
       "      <td>0.54949</td>\n",
       "      <td>0.45163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2</th>\n",
       "      <td>0.19504</td>\n",
       "      <td>0.55094</td>\n",
       "      <td>0.52185</td>\n",
       "      <td>0.51047</td>\n",
       "      <td>0.46282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total predictions</th>\n",
       "      <td>37000.00000</td>\n",
       "      <td>37000.00000</td>\n",
       "      <td>37000.00000</td>\n",
       "      <td>37000.00000</td>\n",
       "      <td>37000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positives</th>\n",
       "      <td>1199.00000</td>\n",
       "      <td>2721.00000</td>\n",
       "      <td>2427.00000</td>\n",
       "      <td>2437.00000</td>\n",
       "      <td>2353.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positives</th>\n",
       "      <td>9538.00000</td>\n",
       "      <td>1973.00000</td>\n",
       "      <td>827.00000</td>\n",
       "      <td>1433.00000</td>\n",
       "      <td>3067.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negatives</th>\n",
       "      <td>3801.00000</td>\n",
       "      <td>2279.00000</td>\n",
       "      <td>2573.00000</td>\n",
       "      <td>2563.00000</td>\n",
       "      <td>2647.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True negatives</th>\n",
       "      <td>22462.00000</td>\n",
       "      <td>30027.00000</td>\n",
       "      <td>31173.00000</td>\n",
       "      <td>30567.00000</td>\n",
       "      <td>28933.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   LogisticRegression     AdaBoost  RandomForest  \\\n",
       "Accuracy                      0.63949      0.88508       0.90811   \n",
       "Precision                     0.11167      0.57968       0.74585   \n",
       "Recall                        0.23980      0.54420       0.48540   \n",
       "F1                            0.15238      0.56138       0.58808   \n",
       "F2                            0.19504      0.55094       0.52185   \n",
       "Total predictions         37000.00000  37000.00000   37000.00000   \n",
       "True positives             1199.00000   2721.00000    2427.00000   \n",
       "False positives            9538.00000   1973.00000     827.00000   \n",
       "False negatives            3801.00000   2279.00000    2573.00000   \n",
       "True negatives            22462.00000  30027.00000   31173.00000   \n",
       "\n",
       "                   DecisionTree  K-NearestNeighbors  \n",
       "Accuracy                0.89200             0.84557  \n",
       "Precision               0.62972             0.43413  \n",
       "Recall                  0.48740             0.47060  \n",
       "F1                      0.54949             0.45163  \n",
       "F2                      0.51047             0.46282  \n",
       "Total predictions   37000.00000         37000.00000  \n",
       "True positives       2437.00000          2353.00000  \n",
       "False positives      1433.00000          3067.00000  \n",
       "False negatives      2563.00000          2647.00000  \n",
       "True negatives      30567.00000         28933.00000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################\n",
    "# All Classifiers over the selected features list \n",
    "#######################################################\n",
    "RF_dict = {\"LogisticRegression\": {'Accuracy': 0.63949,\n",
    "            'Precision': 0.11167,\n",
    "            'Recall': 0.23980,\n",
    "            'F1': 0.15238,\n",
    "            'F2': 0.19504,\n",
    "            'Total predictions': 37000,\n",
    "            'True positives': 1199,\n",
    "            'False positives': 9538,\n",
    "            'False negatives': 3801,\n",
    "            'True negatives': 22462},\n",
    "\"AdaBoost\": {'Accuracy': 0.88508,\n",
    "            'Precision': 0.57968,\n",
    "            'Recall': 0.54420,\n",
    "            'F1': 0.56138,\n",
    "            'F2': 0.55094,\n",
    "            'Total predictions': 37000,\n",
    "            'True positives': 2721,\n",
    "            'False positives': 1973,\n",
    "            'False negatives': 2279,\n",
    "            'True negatives': 30027},\n",
    "\"RandomForest\": {'Accuracy': 0.90811,\n",
    "            'Precision': 0.74585,\n",
    "            'Recall': 0.48540,\n",
    "            'F1': 0.58808,\n",
    "            'F2': 0.52185,\n",
    "            'Total predictions': 37000,\n",
    "            'True positives': 2427,\n",
    "            'False positives':  827,\n",
    "            'False negatives': 2573,\n",
    "            'True negatives': 31173},\n",
    "\"DecisionTree\": {'Accuracy': 0.89200,\n",
    "            'Precision': 0.62972,\n",
    "            'Recall': 0.48740,\n",
    "            'F1': 0.54949,\n",
    "            'F2': 0.51047,\n",
    "            'Total predictions': 37000,\n",
    "            'True positives': 2437,\n",
    "            'False positives': 1433,\n",
    "            'False negatives': 2563,\n",
    "            'True negatives': 30567},\n",
    "\"K-NearestNeighbors\": {'Accuracy': 0.84557,\n",
    "            'Precision': 0.43413,\n",
    "            'Recall': 0.47060,\n",
    "            'F1': 0.45163,\n",
    "            'F2': 0.46282,\n",
    "            'Total predictions': 37000,\n",
    "            'True positives': 2353,\n",
    "            'False positives': 3067,\n",
    "            'False negatives': 2647,\n",
    "            'True negatives': 28933}}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(RF_dict)\n",
    "\n",
    "df = df.reindex(index = [\"Accuracy\",\"Precision\", \"Recall\", \"F1\",\n",
    "            \"F2\",\n",
    "            \"Total predictions\",\n",
    "            \"True positives\",\n",
    "            \"False positives\",\n",
    "            \"False negatives\",\n",
    "            \"True negatives\"])\n",
    "\n",
    "df = df[[\"LogisticRegression\", \"AdaBoost\", \"RandomForest\", \"DecisionTree\", \"K-NearestNeighbors\"]]; df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# RANDOMFOREST OVER ALL LISTS\n",
    "#######################################################\n",
    "# RF_dict = {\"SelectKBest\": {\"Accuracy\": 0.84362,\n",
    "#             \"Precision\": 0.37818,\n",
    "#             \"Recall\": 0.24400,\n",
    "#             \"F1\": 0.29662,\n",
    "#             \"F2\": 0.26264,\n",
    "#             \"Total predictions\": 37000,\n",
    "#             \"True positives\": 1220,\n",
    "#             \"False positives\": 2006,\n",
    "#             \"False negatives\": 3780,\n",
    "#             \"True negatives\": 29994},\n",
    "# \"Lasso\": {\"Accuracy\": 0.81789,\n",
    "#             \"Precision\": 0.22290,\n",
    "#             \"Recall\": 0.13980,\n",
    "#             \"F1\": 0.17183,\n",
    "#             \"F2\": 0.15106,\n",
    "#             \"Total predictions\": 37000,\n",
    "#             \"True positives\": 699,\n",
    "#             \"False positives\": 2437,\n",
    "#             \"False negatives\": 4301,\n",
    "#             \"True negatives\": 29563},\n",
    "# \"RandomForest\": {\"Accuracy\": 0.83377,\n",
    "#             \"Precision\": 0.35877,\n",
    "#             \"Recall\": 0.20780,\n",
    "#             \"F1\": 0.26317,\n",
    "#             \"F2\": 0.22690,\n",
    "#             \"Total predictions\": 35000,\n",
    "#             \"True positives\": 1039,\n",
    "#             \"False positives\": 1857,\n",
    "#             \"False negatives\": 3961,\n",
    "#             \"True negatives\": 28143},\n",
    "# \"DecisionTree\": {\"Accuracy\": 0.90926,\n",
    "#             \"Precision\": 0.70630,\n",
    "#             \"Recall\": 0.50020,\n",
    "#             \"F1\": 0.58565,\n",
    "#             \"F2\": 0.53120,\n",
    "#             \"Total predictions\": 39000,\n",
    "#             \"True positives\": 2501,\n",
    "#             \"False positives\": 1040,\n",
    "#             \"False negatives\": 2499,\n",
    "#             \"True negatives\": 32960},\n",
    "# \"RF_clf_on_DT_list\": {'Accuracy': 0.90811,\n",
    "#             'Precision': 0.74585,\n",
    "#             'Recall': 0.48540,\n",
    "#             'F1': 0.58808,\n",
    "#             'F2': 0.52185,\n",
    "#             'Total predictions': 37000,\n",
    "#             'True positives': 2427,\n",
    "#             'False positives':  827,\n",
    "#             'False negatives': 2573,\n",
    "#             'True negatives': 31173}}\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(RF_dict)\n",
    "\n",
    "# df = df.reindex(index = [\"Accuracy\",\"Precision\", \"Recall\", \"F1\",\n",
    "#             \"F2\",\n",
    "#             \"Total predictions\",\n",
    "#             \"True positives\",\n",
    "#             \"False positives\",\n",
    "#             \"False negatives\",\n",
    "#             \"True negatives\"])\n",
    "\n",
    "# df = df[[\"SelectKBest\", \"Lasso\", \"RandomForest\", \"DecisionTree\"]]; df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# DECISIONTREE OVER ALL LISTS\n",
    "######################################################\n",
    "# DT_dict = {\"all features\": {'Accuracy': 0.87107,\n",
    "#             'Precision': 0.56409,\n",
    "#             'Recall': 0.52367,\n",
    "#             'F1': 0.54313,\n",
    "#             'F2': 0.53128,\n",
    "#             'Total predictions': 41000,\n",
    "#             'True positives': 3142,\n",
    "#             'False positives': 2428,\n",
    "#             'False negatives': 2858,\n",
    "#             'True negatives': 32572},\n",
    "# \"SelectKBest\": {'Accuracy': 0.85512,\n",
    "#             'Precision': 0.50545,\n",
    "#             'Recall': 0.46333,\n",
    "#             'F1': 0.48348,\n",
    "#             'F2': 0.47119,\n",
    "#             'Total predictions': 41000,\n",
    "#             'True positives': 2780,\n",
    "#             'False positives': 2720,\n",
    "#             'False negatives': 3220,\n",
    "#             'True negatives': 32280},\n",
    "# \"Lasso\": {'Accuracy': 0.85941,\n",
    "#             'Precision': 0.45643,\n",
    "#             'Recall': 0.21160,\n",
    "#             'F1': 0.28915,\n",
    "#             'F2': 0.23703,\n",
    "#             'Total predictions': 37000,\n",
    "#             'True positives': 1058,\n",
    "#             'False positives': 1260,\n",
    "#             'False negatives': 3942,\n",
    "#             'True negatives': 30740},\n",
    "# \"RandomForest\": {'Accuracy': 0.84477,\n",
    "#             'Precision': 0.41316,\n",
    "#             'Recall': 0.20600,\n",
    "#             'F1': 0.27492,\n",
    "#             'F2': 0.22896,\n",
    "#             'Total predictions': 35000,\n",
    "#             'True positives': 1030,\n",
    "#             'False positives': 1463,\n",
    "#             'False negatives': 3970,\n",
    "#             'True negatives': 28537},\n",
    "# \"DecisionTree\": {'Accuracy': 0.89200,\n",
    "#             'Precision': 0.62972,\n",
    "#             'Recall': 0.48740,\n",
    "#             'F1': 0.54949,\n",
    "#             'F2': 0.51047,\n",
    "#             'Total predictions': 37000,\n",
    "#             'True positives': 2437,\n",
    "#             'False positives': 1433,\n",
    "#             'False negatives': 2563,\n",
    "#             'True negatives': 30567},\n",
    "# \"RF_clf_on_DT_list\": {'Accuracy': 0.89456,\n",
    "#             'Precision': 0.61111,\n",
    "#             'Recall': 0.48840,\n",
    "#             'F1': 0.54291,\n",
    "#             'F2': 0.50883,\n",
    "#             'Total predictions': 39000,\n",
    "#             'True positives': 2442,\n",
    "#             'False positives': 1554,\n",
    "#             'False negatives': 2558,\n",
    "#             'True negatives': 32446},\n",
    "# \"RF_clf_on_DT_list2\": {'Accuracy': 0.89200,\n",
    "#             'Precision': 0.62972,\n",
    "#             'Recall': 0.48740,\n",
    "#             'F1': 0.54949,\n",
    "#             'F2': 0.51047,\n",
    "#             'Total predictions': 37000,\n",
    "#             'True positives': 2437,\n",
    "#             'False positives': 1433,\n",
    "#             'False negatives': 2563,\n",
    "#             'True negatives': 30567}}\n",
    "# df = pd.DataFrame(DT_dict)\n",
    "\n",
    "# df = df.reindex(index = [\"Accuracy\",\"Precision\", \"Recall\", \"F1\",\n",
    "#             \"F2\",\n",
    "#             \"Total predictions\",\n",
    "#             \"True positives\",\n",
    "#             \"False positives\",\n",
    "#             \"False negatives\",\n",
    "#             \"True negatives\"])\n",
    "\n",
    "# df = df[[\"SelectKBest\", \"Lasso\", \"RandomForest\", \"DecisionTree\"]]; df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################################################\n",
    "# # K-Nearest Neighbors OVER ALL LISTS\n",
    "# #######################################################\n",
    "# DT_dict = {\"RF_clf_on_DT_list\": {'Accuracy': 0.86133,\n",
    "#             'Precision': 0.46083,\n",
    "#             'Recall': 0.48000,\n",
    "#             'F1': 0.47022,\n",
    "#             'F2': 0.47604,\n",
    "#             'Total predictions': 39000,\n",
    "#             'True positives': 2400,\n",
    "#             'False positives': 2808,\n",
    "#             'False negatives': 2600,\n",
    "#             'True negatives': 31192},\n",
    "# \"SelectKBest\": {'Accuracy': 0.84727,\n",
    "#             'Precision': 0.40480,\n",
    "#             'Recall': 0.27680,\n",
    "#             'F1': 0.32878,\n",
    "#             'F2': 0.29549,\n",
    "#             'Total predictions': 37000,\n",
    "#             'True positives': 1384,\n",
    "#             'False positives': 2035,\n",
    "#             'False negatives': 3616,\n",
    "#             'True negatives': 29965},\n",
    "# \"Lasso\": {'Accuracy': 0.77438,\n",
    "#             'Precision': 0.22620,\n",
    "#             'Recall': 0.27660,\n",
    "#             'F1': 0.24888,\n",
    "#             'F2': 0.26480,\n",
    "#             'Total predictions': 37000,\n",
    "#             'True positives': 1383,\n",
    "#             'False positives': 4731,\n",
    "#             'False negatives': 3617,\n",
    "#             'True negatives': 27269},\n",
    "# \"RandomForest\": {'Accuracy': 0.81971,\n",
    "#             'Precision': 0.34102,\n",
    "#             'Recall': 0.28100,\n",
    "#             'F1': 0.30811,\n",
    "#             'F2': 0.29125,\n",
    "#             'Total predictions': 35000,\n",
    "#             'True positives': 1405,\n",
    "#             'False positives': 2715,\n",
    "#             'False negatives': 3595,\n",
    "#             'True negatives': 27285},\n",
    "# \"DecisionTree\": {'Accuracy': 0.86133,\n",
    "#             'Precision': 0.46083,\n",
    "#             'Recall': 0.48000,\n",
    "#             'F1': 0.47022,\n",
    "#             'F2': 0.47604,\n",
    "#             'Total predictions': 39000,\n",
    "#             'True positives': 2400,\n",
    "#             'False positives': 2808,\n",
    "#             'False negatives': 2600,\n",
    "#             'True negatives': 31192}\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(DT_dict)\n",
    "\n",
    "# df = df.reindex(index = [\"Accuracy\",\"Precision\", \"Recall\", \"F1\",\n",
    "#             \"F2\",\n",
    "#             \"Total predictions\",\n",
    "#             \"True positives\",\n",
    "#             \"False positives\",\n",
    "#             \"False negatives\",\n",
    "#             \"True negatives\"])\n",
    "\n",
    "# df = df[[\"SelectKBest\", \"Lasso\", \"RandomForest\", \"DecisionTree\", \"RF_clf_on_DT_list\"]]; df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
